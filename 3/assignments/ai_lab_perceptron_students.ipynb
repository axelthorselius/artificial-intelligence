{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sacred-connecticut",
   "metadata": {},
   "source": [
    "# Classification with the perceptron and logistic regression\n",
    "\n",
    "__Individual assignment__\n",
    "\n",
    "Author of the assignment: Pierre Nugues\n",
    "\n",
    "__Student name__: Axel Thorselius (ax1744th-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-jackson",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The objectives of this second assignment are to:\n",
    "\n",
    "1.  Write a linear regression program using gradient descent;\n",
    "2.  Write linear classifiers using the perceptron algorithm and logistic regression;\n",
    "3.  Experiment variations of the algorithms;\n",
    "4.  Evaluate your classifiers;\n",
    "5.  Experiment with popular tools;\n",
    "6.  Read a scientific article on optimization techniques and comment it;\n",
    "7.  Present your code, results, and comments in a short dissertation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-coral",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The gradient descent is a basic technique to estimate linear discriminant functions. You will first use the gradient descent method to implement linear regression. You will then program the perceptron algorithm. Finally, you will improve the threshold function with the logistic curve (logistic regression). You will try various configurations and study their influence on the learning speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-external",
   "metadata": {},
   "source": [
    "##  Programming language\n",
    "As programming language, you will use Python and write your code in this notebook.\n",
    "\n",
    "You need to have a comprehensive Python distribution such as Anaconda (https://www.anaconda.com/products/individual). This distribution is available on the student computers at the computer science department.\n",
    "Finally, you start a notebook by typing:\n",
    "\n",
    "`jupyter lab`\n",
    "\n",
    "in a terminal window and you select the notebook by clicking on it in the left pane.\n",
    "You run the pieces of code by typing shift+enter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-crawford",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Imports you may use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import gradient_descent_numpy as gdn\n",
    "import random\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-genealogy",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "You will implement the gradient descent method as explained in pages 719--720 in Russell-Norvig and in the slides to compute regression lines. You will implement the stochastic and batch versions of the algorithm.\n",
    "\n",
    "You must try to do it yourself first. If you encounter difficulties, you also have the solution to this exercise in the section _Solution to linear regression_ below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-basic",
   "metadata": {},
   "source": [
    "### Your implementation of linear regression\n",
    "You will implement a regression program to predict the counts of _A_'s in a text from the total count of letters. You will apply it on two data sets corresponding to letter counts in the 15 chapters of the French and English versions of _Salammb√¥_, where the first column is the total count of characters and the second one, the count of A's. \n",
    "\n",
    "Start with either French or English and when your program ready, test it on the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_fr = np.array([[36961, 2503],\n",
    "                      [43621, 2992],\n",
    "                      [15694, 1042],\n",
    "                      [36231, 2487],\n",
    "                      [29945, 2014],\n",
    "                      [40588, 2805],\n",
    "                      [75255, 5062],\n",
    "                      [37709, 2643],\n",
    "                      [30899, 2126],\n",
    "                      [25486, 1784],\n",
    "                      [37497, 2641],\n",
    "                      [40398, 2766],\n",
    "                      [74105, 5047],\n",
    "                      [76725, 5312],\n",
    "                      [18317, 1215]])\n",
    "\n",
    "stat_en = np.array([[35680, 2217],\n",
    "                      [42514, 2761],\n",
    "                      [15162, 990],\n",
    "                      [35298, 2274],\n",
    "                      [29800, 1865],\n",
    "                      [40255, 2606],\n",
    "                      [74532, 4805],\n",
    "                      [37464, 2396],\n",
    "                      [31030, 1993],\n",
    "                      [24843, 1627],\n",
    "                      [36172, 2375],\n",
    "                      [39552, 2560],\n",
    "                      [72545, 4597],\n",
    "                      [75352, 4871],\n",
    "                      [18031, 1119]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-backing",
   "metadata": {},
   "source": [
    "From the datasets above, tell what is $\\mathbf{X}$ and $\\mathbf{y}$. Extract:\n",
    "1. The $\\mathbf{X}$ matrix, where you will have a column to model the intercept;\n",
    "2. The $\\mathbf{y}$ vector\n",
    "\n",
    "from these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = np.ones(stat_en.shape)\n",
    "\n",
    "X_en[:, 1] = stat_en[:, 0]\n",
    "\n",
    "y_en = np.array(stat_en[:, 1]).reshape(-1, 1)\n",
    "\n",
    "X_fr = np.ones(stat_fr.shape)\n",
    "\n",
    "X_fr[:, 1] = stat_fr[:, 0]\n",
    "\n",
    "y_fr = np.array(stat_fr[:, 1]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-neighborhood",
   "metadata": {},
   "source": [
    "Scale the arrays so that they fit in the range [0, 1] on the $x$ and $y$ axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "X_en, maxima_X_en = gdn.normalize(X_en)\n",
    "\n",
    "y_en, maxima_y_en = gdn.normalize(y_en)\n",
    "\n",
    "maxima_en = np.concatenate((maxima_X_en, maxima_y_en))\n",
    "\n",
    "X_fr, maxima_X_fr = gdn.normalize(X_fr)\n",
    "\n",
    "y_fr, maxima_y_fr = gdn.normalize(y_fr)\n",
    "\n",
    "maxima_fr = np.concatenate((maxima_X_fr, maxima_y_fr))\n",
    "\n",
    "# Xmax = np.max(X)\n",
    "# for i in range (X.shape[0]):\n",
    "#     X[i][1] /= Xmax\n",
    "# print(X)\n",
    "\n",
    "# ymax = max(y)\n",
    "# y = y/ymax\n",
    "\n",
    "# print(y)\n",
    "# X_fr_scaled = X_fr/(max(X_fr))\n",
    "# y_fr_scaled = y_fr/(max(y_fr))\n",
    "# print(X_fr_scaled)\n",
    "# print(y_fr_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-upset",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-supplier",
   "metadata": {},
   "source": [
    "Implement the descent functions. You will pass `X`, `y`, the learning rate in the $\\alpha$ variable, the initial weight vector in `w`, the tolerance in the $\\epsilon$ variable, the maximal number of epochs in `epochs`. You will return `w`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-luther",
   "metadata": {},
   "source": [
    "Batch descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def fit_batch(X, y, alpha, w,\n",
    "                  epochs=500,\n",
    "                  epsilon=1.0e-5):\n",
    "\n",
    "    alpha /= len(X)\n",
    "    for e in range (epochs):\n",
    "        gradient = np.dot(X.T, (y - np.dot(X, w)))\n",
    "        w += alpha * gradient\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "    # while epochs > 0 and np.linalg.norm(gradient) > epsilon:\n",
    "    #     for x, y in zip(X, Y):\n",
    "    #         y_hat = np.dot(w, np.array([1, x]))\n",
    "    #         gradient = np.array([1, x]) * (y - y_hat)\n",
    "    #         w += alpha * gradient/len(X)\n",
    "            \n",
    "    #     epochs -= 1\n",
    "    # return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-sight",
   "metadata": {},
   "source": [
    "Stochastic descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stoch(X, y, alpha, w,\n",
    "                  epochs=500,\n",
    "                  epsilon=1.0e-5):\n",
    "    idx = list(range(len(X)))\n",
    "    random.seed(0)\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(idx)\n",
    "        for i in idx:\n",
    "            gradient = (np.dot(X[i], w) - y[i])[0] * X[i:i + 1].T\n",
    "            w -= alpha * gradient\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write your code here\n",
    "# def fit_stoch(X, y, alpha, w,\n",
    "#                   epochs=500,\n",
    "#                   epsilon=1.0e-5):\n",
    "    \n",
    "#     while epochs > 0:\n",
    "#         for i in range(len(X)):\n",
    "#             gradient = (y[i] - np.dot(X[i], w)) * X[i]\n",
    "#             w += (alpha/len(X)) * gradient\n",
    "#             if np.linalg.norm(gradient) < epsilon:\n",
    "#                 return w\n",
    "#         epochs -= 1\n",
    "    \n",
    "#     return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-kazakhstan",
   "metadata": {},
   "source": [
    "#### Applying batch descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-assets",
   "metadata": {},
   "source": [
    "Apply the batch descent and print the final weight values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# print(X_fr)\n",
    "# print(y_fr)\n",
    "w_en = fit_batch(X_en, y_en, 1, np.zeros((2, 1)))\n",
    "maxima_en = maxima_en.reshape(-1, 1)\n",
    "w_en = maxima_en[-1, 0] * (w_en / maxima_en[:-1, 0:1])\n",
    "\n",
    "w_fr = fit_batch(X_fr, y_fr, 1, np.zeros((2, 1)))\n",
    "maxima_fr = maxima_fr.reshape(-1, 1)\n",
    "w_fr = maxima_fr[-1, 0] * (w_fr / maxima_fr[:-1, 0:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-blast",
   "metadata": {},
   "source": [
    "Visualize the points of your dataset as well as the regression lines you obtain using matplotlib or another similar program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "plt.plot(stat_en[:,0], stat_en[:, 1], 'ro', label=\"English\")\n",
    "x_en = np.linspace(0, maxima_X_en, 10000)\n",
    "y_tmp_en = w_en[1]*x_en+w_en[0]\n",
    "plt.plot(x_en, y_tmp_en, 'r')\n",
    "\n",
    "print(\"w_en: \", w_en)\n",
    "\n",
    "plt.plot(stat_fr[:,0], stat_fr[:, 1], 'bo', label=\"French\")\n",
    "x_fr = np.linspace(0, maxima_X_fr, 10000)\n",
    "y_tmp_fr = w_fr[1]*x_fr+w_fr[0]\n",
    "plt.plot(x_fr, y_tmp_fr, 'b')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "print(\"w_fr: \", w_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-tampa",
   "metadata": {},
   "source": [
    "#### Stochastic descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "w_en = fit_stoch(X_en, y_en, 1, np.zeros((2, 1)))\n",
    "w_en = maxima_en[-1, 0] * (w_en / maxima_en[:-1, 0:1])\n",
    "\n",
    "w_fr = fit_stoch(X_fr, y_fr, 1, np.zeros((2, 1)))\n",
    "w_fr = maxima_fr[-1, 0] * (w_fr / maxima_fr[:-1, 0:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-circular",
   "metadata": {},
   "source": [
    "Visualize the points of your dataset as well as the regression lines you obtain using matplotlib or another similar program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "plt.plot(stat_en[:,0], stat_en[:, 1], 'ro', label=\"English\")\n",
    "x_en = np.linspace(0, maxima_X_en, 10000)\n",
    "y_tmp_en = w_en[1]*x_en+w_en[0]\n",
    "plt.plot(x_en, y_tmp_en, 'r')\n",
    "\n",
    "print(\"w_en: \", w_en)\n",
    "\n",
    "plt.plot(stat_fr[:,0], stat_fr[:, 1], 'bo', label=\"French\")\n",
    "x_fr = np.linspace(0, maxima_X_fr, 10000)\n",
    "y_tmp_fr = w_fr[1]*x_fr+w_fr[0]\n",
    "plt.plot(x_fr, y_tmp_fr, 'b')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "print(\"w_fr: \", w_fr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-assignment",
   "metadata": {},
   "source": [
    "### A solution to linear regression\n",
    "\n",
    "To help you start this assignment, your instructor wrote two Python notebooks that solve this exercise on linear regression. You can find them here: https://github.com/pnugues/ilppp/tree/master/programs/ch04/python\n",
    "The first notebook, `gradient_descent.ipynb`, only uses Python and vector operations such as the dot product that are in the `vector.py` file.\n",
    "The second notebook, `gradient_descent_numpy.ipynb`, uses Numpy. It is more compact, but you need to know a bit of numpy.\n",
    "\n",
    "To run these programs, download them on your computer as well as the other program in the import list: vector.py\n",
    "\n",
    "The programs are also available as Python programs from\n",
    "https://github.com/pnugues/ilppp/tree/master/programs/ch04/python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-province",
   "metadata": {},
   "source": [
    "## Classification\n",
    "You will use the same data set as for linear regression, but this time to classify a chapter as French or English. Given a pair of numbers corresponding the letter count and count of _A_, you will predict the language:\n",
    "1. $\\mathbf{x} = (35680, 2217)$ $\\to$ $y$ = English\n",
    "2. $\\mathbf{x} = (37497, 2641)$ $\\to$ $y$ = French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-psychology",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "You will use the arrays below:\n",
    "1. `X` contains the counts of letters and of _A_ s as well as a column of ones for the intercept;\n",
    "2. `y` contains the classes, where 0 is for English and 1 for French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1.0, 35680.0, 2217.0],\n",
    "     [1.0, 42514.0, 2761.0],\n",
    "     [1.0, 15162.0, 990.0],\n",
    "     [1.0, 35298.0, 2274.0],\n",
    "     [1.0, 29800.0, 1865.0],\n",
    "     [1.0, 40255.0, 2606.0],\n",
    "     [1.0, 74532.0, 4805.0],\n",
    "     [1.0, 37464.0, 2396.0],\n",
    "     [1.0, 31030.0, 1993.0],\n",
    "     [1.0, 24843.0, 1627.0],\n",
    "     [1.0, 36172.0, 2375.0],\n",
    "     [1.0, 39552.0, 2560.0],\n",
    "     [1.0, 72545.0, 4597.0],\n",
    "     [1.0, 75352.0, 4871.0],\n",
    "     [1.0, 18031.0, 1119.0],\n",
    "     [1.0, 36961.0, 2503.0],\n",
    "     [1.0, 43621.0, 2992.0],\n",
    "     [1.0, 15694.0, 1042.0],\n",
    "     [1.0, 36231.0, 2487.0],\n",
    "     [1.0, 29945.0, 2014.0],\n",
    "     [1.0, 40588.0, 2805.0],\n",
    "     [1.0, 75255.0, 5062.0],\n",
    "     [1.0, 37709.0, 2643.0],\n",
    "     [1.0, 30899.0, 2126.0],\n",
    "     [1.0, 25486.0, 1784.0],\n",
    "     [1.0, 37497.0, 2641.0],\n",
    "     [1.0, 40398.0, 2766.0],\n",
    "     [1.0, 74105.0, 5047.0],\n",
    "     [1.0, 76725.0, 5312.0],\n",
    "     [1.0, 18317.0, 1215.0]]\n",
    "y = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "     1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-looking",
   "metadata": {},
   "source": [
    "We visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fr = [x[1] for i, x in enumerate(X) if y[i] == 1]\n",
    "y_fr = [x[2] for i, x in enumerate(X) if y[i] == 1]\n",
    "x_en = [x[1] for i, x in enumerate(X) if y[i] == 0]\n",
    "y_en = [x[2] for i, x in enumerate(X) if y[i] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fr, y_fr, color='red')\n",
    "plt.scatter(x_en, y_en, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-pointer",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-reporter",
   "metadata": {},
   "source": [
    "Gradient descent algorithms can be very sensitive to the range. Therefore, we normalize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(observations):\n",
    "    maxima = [max([obs[i] for obs in observations]) for i in range(len(observations[0]))]\n",
    "    return ([[obs[i] / maxima[i]\n",
    "              for i in range(len(observations[0]))] for obs in observations],\n",
    "            maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm, maxima = normalize(X)\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-reverse",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "1. Write the perceptron program as explained in pages 723--725 in Russell-Norvig and in the slides and run it on your data set. As suggested program structure, use two functions: \n",
    " * `fit(X, y)` that will return `w` (the model). You can choose a stochastic or batch variant;\n",
    " * `predict(X, w)` that will return `y_hat`. You can encapsulate these functions in a class and, of course, add more parameters.\n",
    "2. As a stop criterion, you will use the number of misclassified examples.\n",
    "3. You will report the parameters you have used and the weight vector\n",
    "\n",
    "You can use numpy or not. The next cells are just suggested steps. You can implement it your way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-missouri",
   "metadata": {},
   "source": [
    "### The `predict(X, w)` function\n",
    "Write a `predict(X, w)` function that given a matrix of observations $\\mathbf{X}$ and a weight vector $\\mathbf{w}$ will return a $\\mathbf{\\hat{y}}$ vector classes (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Does not follow the above description.\n",
    "# Takes row from X and w and returns 1 if resulting scalar is > 0, else 0\n",
    "# p.701 4th ed\n",
    "def predict(X, w):\n",
    "    if np.dot(w, X) >= 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-morgan",
   "metadata": {},
   "source": [
    "### The `fit(X, y)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-cartridge",
   "metadata": {},
   "source": [
    "Write a `fit(X, y)` function that given a matrix of observations $\\mathbf{X}$ and a vector of responses $\\mathbf{y}$ will return a weight $\\mathbf{w}$ vector. You may use the other arguments of the function, notably the number of misclassified examples to define the stop condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import random\n",
    "import vector\n",
    "def fit_stoch(X, y,\n",
    "              epochs=1000,\n",
    "              max_misclassified=0,\n",
    "              verbose=True):\n",
    "    w = np.ones(np.size(X[0]))\n",
    "    idx = list(range(len(X)))\n",
    "    # missclassified = 0\n",
    "    alpha = 10\n",
    "    random.seed(11)\n",
    "    for epoch in range(epochs):\n",
    "        missclassified = 0\n",
    "        random.shuffle(idx)\n",
    "        for i in idx:\n",
    "            predict_var = predict(X[i], w)\n",
    "            # print(\"w\", w)\n",
    "            # print(\"alpha\", alpha)\n",
    "            # print(\"y[i]\", y[i])\n",
    "            # print(\"predict\", predict_var)\n",
    "            # print(\"X[i]\", X[i])\n",
    "            w += alpha * np.dot((y[i] - predict_var), X[i])\n",
    "            if predict_var != y[i]:\n",
    "                missclassified += 1\n",
    "        if missclassified <= max_misclassified:\n",
    "            break\n",
    "\n",
    "    return w\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-graduate",
   "metadata": {},
   "source": [
    "### Fitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = fit_stoch(X_norm, y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [w[i] * maxima[-1] / maxima[i] for i in range(len(w))]\n",
    "print(\"Restored weights\", w)\n",
    "w = [w[j] / w[-1] for j in range(len(w))]\n",
    "print(\"Weights with y set to 1\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-control",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fr, y_fr, color='blue', label=\"French\")\n",
    "plt.scatter(x_en, y_en, color='red', label=\"English\")\n",
    "plt.plot([min(x_fr + x_en), max(x_fr + x_en)],\n",
    "             [-w[1] * min(x_fr + x_en) - w[0], -w[1] * max(x_fr + x_en) - w[0]], label=\"Boundary\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-honolulu",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Evaluate your perceptron using the leave-one-out cross validation method. You will have to train and run 30 models. In each train/run session, you will train on 29 samples and evaluate on the remaining sample. You have then either a correct or a wrong classification. You will sum these classifications, i.e. the number of correct classifications, to get your final evaluation, for instance 29/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def leave_one_out_cross_val(X, y, fitting_function):\n",
    "    n = len(y)\n",
    "    errs = 0\n",
    "    for i in range (n):\n",
    "        test_set = X[i]\n",
    "        # training_set = np.delete(X[i])\n",
    "        training_set = X[:i] + X[i+1:]\n",
    "        w = fitting_function(training_set, y)\n",
    "        if predict(test_set, w) != y[i]:\n",
    "            errs += 1\n",
    "    \n",
    "    return (n - errs) / n\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoch_accuracy = leave_one_out_cross_val(X_norm, y, fit_stoch)\n",
    "print('Cross-validation accuracy (stochastic):', stoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-score",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "From your perceptron program, implement logistic regression. You can either follow the description from the textbook, S. Russell and R. Norvig, _Artificial Intelligence_, 2010, pages 725--727, or the slides. You can either implement the stochastic or the batch version of the algorithm, or both versions. As stop criterion, you will use either the norm of the gradient or the norm of the difference between two consecutive weight vectors. You will also set a maximal number of epochs. Run the resulting program on your data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-albania",
   "metadata": {},
   "source": [
    "Write the logistic function, where the $x$ input is a real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-payroll",
   "metadata": {},
   "source": [
    "### The `predict(X, w)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-mustang",
   "metadata": {},
   "source": [
    "Write a `predict_proba()` function that given a matrix of observations $\\mathbf{X}$ and a weight vector $\\mathbf{w}$ will return a vector of probabilities to belong to class 1: The vector will consist of $P(1|\\mathbf{x}_i)$ for all the $i$ rows of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def predict_proba(X, w):\n",
    "    return np.array([logistic(np.dot(x, w)) for x in X])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-salad",
   "metadata": {},
   "source": [
    "Write a `predict(X, w)` function that given a matrix of observations $\\mathbf{X}$ and a weight vector $\\mathbf{w}$ will return the class. You will use `predict_proba()` and set the threshold to belong to class 1 to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def predict(X, w):\n",
    "    return np.array([1 if prob > 0.5 else 0 for prob in predict_proba(X, w)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-variation",
   "metadata": {},
   "source": [
    "### The `fit(X, y)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-relation",
   "metadata": {},
   "source": [
    "You will now write the `fit(X, y)` function as with the perceptron. You may call it `fit_stoch(X, y)` or `fit_batch(X, y)`. Use the parameters given in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import random\n",
    "def fit_stoch(X, y, alpha=100,\n",
    "              epochs=1000,\n",
    "              epsilon=1.0e-4,\n",
    "              verbose=False):\n",
    "    w = np.zeros(np.size(X[0]))\n",
    "    idx = list(range(len(X)))\n",
    "    random.seed(0)\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(idx)\n",
    "        for i in idx:\n",
    "            # predict_var = logistic(np.dot(X[i], w))\n",
    "            predict_var = predict_proba([X[i]], w)[0]\n",
    "            gradient = np.dot((y[i] - predict_var), X[i])\n",
    "            w += alpha * gradient\n",
    "        # Something here is wrong\n",
    "        # if np.linalg.norm(gradient) < epsilon:\n",
    "        #     break\n",
    "    print(epoch)\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = fit_stoch(X_norm, y, verbose=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [w[i] / maxima[i] for i in range(len(w))]\n",
    "print(\"Restored weights\", w)\n",
    "w = [w[j] / w[-1] for j in range(len(w))]\n",
    "print(\"Weights with y set to 1\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-motion",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_fr, y_fr, color='blue', label=\"French\")\n",
    "plt.scatter(x_en, y_en, color='red', label=\"English\")\n",
    "plt.plot([min(x_fr + x_en), max(x_fr + x_en)],\n",
    "         [-w[1] * min(x_fr + x_en) - w[0], -w[1] * max(x_fr + x_en) - w[0]], label=\"Boundary\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-value",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Evaluate your logistic regression using the leave-one-out cross validation method as with the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def leave_one_out_cross_val(X, y, fitting_function):\n",
    "    n = len(y)\n",
    "    errs = 0\n",
    "    for i in range (n):\n",
    "        test_set = X[i]\n",
    "        training_set = X[:i] + X[i+1:]\n",
    "        w = fitting_function(training_set, y)\n",
    "        if logistic(np.dot(test_set, w)) > 0.5 and y[i] != 1:\n",
    "            errs += 1\n",
    "    \n",
    "    return (n - errs) / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoch_accuracy = leave_one_out_cross_val(X_norm, y, fit_stoch)\n",
    "print('Cross-validation accuracy (batch):', stoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-difficulty",
   "metadata": {},
   "source": [
    "## Visualizing the logistic surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_surf(x_range, y_range, w_opt):\n",
    "    z_axis = np.array([[0.0] * len(y_range) for i in range(len(x_range))])\n",
    "    x_axis, y_axis = np.meshgrid(x_range, y_range)\n",
    "    z_axis = z_axis.reshape(x_axis.shape)\n",
    "\n",
    "    # We compute the probability surface as a function of x and y\n",
    "    for i in range(len(x_range)):\n",
    "        for j in range(len(y_range)):\n",
    "            z_axis[j, i] = logistic(np.dot([1, x_range[i], y_range[j]], w_opt))\n",
    "    return x_axis, y_axis, z_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 100000, 200)\n",
    "y_range = np.linspace(0, 10000, 200)\n",
    "#w = [2.073225839414742, -0.049125455233437906, 0.7440143556104162]\n",
    "\n",
    "x_axis, y_axis, z_axis = plot_logistic_surf(x_range, y_range, w)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "surf = ax.plot_surface(y_axis, x_axis, z_axis, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False, alpha=0.2)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "# We plot the observations\n",
    "for x, y_class in zip(X, y):\n",
    "    if y_class == 1:\n",
    "        ax.scatter(x[2], x[1], y_class, color='green', marker='x')\n",
    "    else:\n",
    "        ax.scatter(x[2], x[1], y_class, color='red', marker='x')\n",
    "\n",
    "ax.elev = 30 \n",
    "ax.azim = -150\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-ideal",
   "metadata": {},
   "source": [
    "## Programming logistic regression with popular APIs\n",
    "Should you use logistic regression in a project, you will probably resort to existing libraries. In the next cells, you will apply the logistic regression classification with two popular APIs:\n",
    "1. sklearn\n",
    "2. Keras\n",
    "\n",
    "`sklearn` is included in anaconda.\n",
    "You will install the rest with:\n",
    "```\n",
    "pip install --upgrade keras tensorflow tensorflow-addons \n",
    "```\n",
    "You will read and run the code in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-karma",
   "metadata": {},
   "source": [
    "All these APIs are built on numpy and we convert the dataset into numpy if you have not done it already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X_norm = np.array(X_norm)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-institution",
   "metadata": {},
   "source": [
    "They also handle the intercept so we do not need the first column of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, 1:]\n",
    "X_norm = X_norm[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-peter",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "Using the dataset of English and French datapoints, we apply logistic regression with the sklearn API. We need the `LogisticRegression` class, the fit() and predict() functions. The weights are in the `coef_` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X, y)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-adventure",
   "metadata": {},
   "source": [
    "We predict the classes of the $\\mathbf{X}$ with the `predict()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-watson",
   "metadata": {},
   "source": [
    "We predict the class probabilities of the $\\mathbf{X}$ with the `predict_proba()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-oxygen",
   "metadata": {},
   "source": [
    "### Keras\n",
    "Using the dataset of English and French datapoints, we apply logistic regression with Keras. We need the `Sequential` and `Dense` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-baseball",
   "metadata": {},
   "source": [
    "We fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=1500, batch_size=4, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.predict(X) > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-premium",
   "metadata": {},
   "source": [
    "If you do not obtain a correct classification, rerun the training with more epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "handmade-involvement",
   "metadata": {},
   "source": [
    "## Reading\n",
    "You will read the article *An overview of gradient descent optimization algorithms* by Ruder (2017) and you will outline the main characteristics of all the optimization algorithms the author describes. This part should be of about one to two pages. Link to the article: https://arxiv.org/abs/1609.04747.\n",
    "\n",
    "You can also visualize the descents of the algorithm variants on Ruder's webpage: https://www.ruder.io/optimizing-gradient-descent/.\n",
    "\n",
    "If you understand French, or using Google translate, you may also want to read the original article on gradient descent by Cauchy here:  https://gallica.bnf.fr/ark:/12148/bpt6k2982c/f540.item.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-student",
   "metadata": {},
   "source": [
    "### Report\n",
    "\n",
    "The assignment must be documented in the report, which should contain the following:\n",
    "\n",
    "*   The name of the author, the title of the assignment, and any relevant information on the front page;\n",
    "*   A presentation of the assignment and the possible improvements you would have brought;\n",
    "*   A presentation of your implementation;\n",
    "*   A print-out of the example set(s) and the resulting weight vectors;\n",
    "*   Comments on the results you have obtained, including your cross validation;\n",
    "*   A short dissertation on the optimization algorithms from Ruder's paper.\n",
    "\n",
    "Please, typeset and format your report consistently. You must use Latex. Documents written using MS Word or any similar format will not be considered.\n",
    "\n",
    "You may have a look at the code in the textbook code repository (or any other implementations), but the code you hand in must be your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-cedar",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submit the notebook and the report to Canvas (two files). Do not include the code printout in the report, but only comments on its interesting parts. You will submit the notebook as a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-occurrence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
